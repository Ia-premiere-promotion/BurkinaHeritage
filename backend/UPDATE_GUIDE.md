# ğŸ‡§ğŸ‡« BurkinaHeritage - Guide de Mise Ã  Jour des DonnÃ©es

Ce guide explique comment mettre Ã  jour le systÃ¨me avec de nouvelles donnÃ©es et faire du web scraping.

## ğŸ“‹ Table des MatiÃ¨res

1. [Scripts Disponibles](#scripts-disponibles)
2. [Mise Ã  Jour ComplÃ¨te (RecommandÃ©)](#mise-Ã -jour-complÃ¨te)
3. [Mise Ã  Jour Ã‰tape par Ã‰tape](#mise-Ã -jour-Ã©tape-par-Ã©tape)
4. [Web Scraping](#web-scraping)
5. [DÃ©pannage](#dÃ©pannage)

---

## ğŸ› ï¸ Scripts Disponibles

### 1. `update_system.py` - Script Principal â­
Orchestre toute la mise Ã  jour automatiquement.

### 2. `prepare_data_csv.py` - Traitement CSV
Traite le fichier `burkinaheritage_corpus_clean.csv` et crÃ©e le corpus.

### 3. `web_scraper.py` - Web Scraping
RÃ©cupÃ¨re du contenu en ligne sur le Burkina Faso (UNESCO, Wikipedia, etc.).

### 4. `rebuild_database.py` - Reconstruction BD
Reconstruit la base de donnÃ©es vectorielle ChromaDB.

---

## ğŸš€ Mise Ã  Jour ComplÃ¨te (RecommandÃ©)

### Option 1: Avec Web Scraping

```bash
cd backend
python update_system.py
```

Le script va :
1. âœ… Traiter les donnÃ©es CSV
2. âœ… Faire du web scraping (avec votre confirmation)
3. âœ… Reconstruire la base de donnÃ©es
4. âœ… VÃ©rifier que tout fonctionne

### Option 2: Sans Web Scraping

```bash
cd backend
python update_system.py --no-scraping
```

Plus rapide, mais sans enrichissement web.

---

## ğŸ“ Mise Ã  Jour Ã‰tape par Ã‰tape

Si vous voulez contrÃ´ler chaque Ã©tape :

### Ã‰tape 1 : Traiter les DonnÃ©es CSV

```bash
cd backend
python prepare_data_csv.py
```

**Ce que Ã§a fait :**
- Lit `Documents/burkinaheritage_corpus_clean.csv`
- Nettoie et filtre les textes
- CrÃ©e `data/corpus.json`
- GÃ©nÃ¨re `data/sources.txt`

**RÃ©sultat attendu :**
```
âœ… XXX documents crÃ©Ã©s
ğŸ’¾ Fichiers sauvegardÃ©s
```

### Ã‰tape 2 : Web Scraping (Optionnel)

```bash
cd backend
python web_scraper.py
```

**Ce que Ã§a fait :**
- Scrape les sites UNESCO sur le Burkina Faso
- Scrape Wikipedia (Culture, Histoire)
- Fusionne avec le corpus existant
- Respecte les dÃ©lais entre requÃªtes

**âš ï¸ PrÃ©requis :**
- Connexion internet active
- Pas de pare-feu bloquant

**RÃ©sultat attendu :**
```
âœ… XX pages scrapÃ©es
ğŸ’¾ Corpus fusionnÃ©
```

### Ã‰tape 3 : Reconstruire la Base de DonnÃ©es

```bash
cd backend
python rebuild_database.py
```

**Ce que Ã§a fait :**
- Sauvegarde l'ancienne BD (backup)
- Supprime l'ancienne BD
- CrÃ©e une nouvelle BD ChromaDB
- Indexe tous les documents du corpus

**âš ï¸ ATTENTION :** Cette opÃ©ration supprime l'ancienne base !

**RÃ©sultat attendu :**
```
âœ… XXX documents ajoutÃ©s Ã  ChromaDB
ğŸ” VÃ©rification: XXX documents dans la collection
```

---

## ğŸŒ Web Scraping - DÃ©tails

### Sites ScrapÃ©s

Le script `web_scraper.py` collecte des donnÃ©es depuis :

1. **UNESCO Burkina Faso**
   - Patrimoine mondial
   - Patrimoine culturel immatÃ©riel

2. **Wikipedia**
   - Culture du Burkina Faso
   - Histoire du Burkina Faso
   - Article gÃ©nÃ©ral sur le Burkina Faso

### Configuration PersonnalisÃ©e

Pour ajouter d'autres sites, modifiez `web_scraper.py` :

```python
self.target_sites = [
    {
        "name": "Votre Site",
        "urls": [
            "https://example.com/page1",
            "https://example.com/page2"
        ],
        "category": "votre-catÃ©gorie"
    }
]
```

### Bonnes Pratiques

âœ… **Ã€ FAIRE :**
- VÃ©rifier le fichier `robots.txt` des sites
- Respecter les dÃ©lais entre requÃªtes (2 secondes minimum)
- Utiliser des User-Agents valides
- Scraper en dehors des heures de pointe

âŒ **Ã€ Ã‰VITER :**
- Scraper trop rapidement (risque de ban IP)
- Ignorer les conditions d'utilisation
- Surcharger les serveurs

---

## ğŸ” VÃ©rification AprÃ¨s Mise Ã  Jour

### VÃ©rifier les Fichiers CrÃ©Ã©s

```bash
ls -lh data/
```

Vous devriez voir :
```
corpus.json           # Le corpus complet
sources.txt           # Liste des sources
scraped_data.json     # DonnÃ©es scrapÃ©es (si scraping)
chroma_db/            # Base de donnÃ©es vectorielle
```

### Tester le Corpus

```bash
python -c "import json; data=json.load(open('data/corpus.json')); print(f'{len(data)} documents')"
```

### Tester ChromaDB

```python
import chromadb
client = chromadb.PersistentClient(path="data/chroma_db")
collection = client.get_collection("burkinaheritage")
print(f"Documents dans ChromaDB: {collection.count()}")
```

### Tester l'API

```bash
# Lancer l'API
python main.py

# Dans un autre terminal, tester
curl -X POST "http://localhost:8000/api/chat" \
  -H "Content-Type: application/json" \
  -d '{"question":"Parle-moi du Burkina Faso","use_llm":false}'
```

---

## ğŸ› DÃ©pannage

### ProblÃ¨me : "Fichier CSV introuvable"

**Solution :**
```bash
# VÃ©rifier que le CSV existe
ls -l Documents/burkinaheritage_corpus_clean.csv

# Si manquant, vÃ©rifiez le chemin
```

### ProblÃ¨me : "Erreur lors du scraping"

**Solutions possibles :**
1. VÃ©rifier la connexion internet
2. VÃ©rifier que les sites sont accessibles
3. Augmenter le timeout dans `web_scraper.py`
4. DÃ©sactiver le scraping : `--no-scraping`

### ProblÃ¨me : "ChromaDB erreur"

**Solutions :**
```bash
# RÃ©installer ChromaDB
pip install --force-reinstall chromadb

# Supprimer et recrÃ©er la BD
rm -rf data/chroma_db
python rebuild_database.py
```

### ProblÃ¨me : "Pas assez de documents"

**Solutions :**
1. Activer le web scraping
2. Ajouter plus de PDFs dans `Documents/`
3. Ajouter plus d'URLs dans `web_scraper.py`
4. RÃ©duire le filtrage dans `prepare_data_csv.py`

### ProblÃ¨me : "MÃ©moire insuffisante"

**Solutions :**
```bash
# Traiter par lots plus petits
# Dans prepare_data_csv.py, rÃ©duire batch_size

# Ou augmenter la mÃ©moire Python
export PYTHONHASHSEED=0
```

---

## ğŸ“Š Statistiques Attendues

AprÃ¨s une mise Ã  jour complÃ¨te, vous devriez avoir :

- **Documents CSV** : ~300-400 documents (selon le CSV)
- **Documents ScrapÃ©s** : ~20-50 documents
- **Total** : ~350-450 documents
- **Taille corpus.json** : ~1-3 MB
- **Taille ChromaDB** : ~5-15 MB

---

## ğŸ”„ Automatisation (Optionnel)

Pour mettre Ã  jour automatiquement chaque semaine :

```bash
# CrÃ©er un cron job
crontab -e

# Ajouter cette ligne (tous les dimanches Ã  2h du matin)
0 2 * * 0 cd /chemin/vers/backend && python update_system.py --no-scraping
```

---

## ğŸ“ Support

En cas de problÃ¨me :

1. VÃ©rifier les logs dans le terminal
2. Consulter `data/sources.txt` pour voir ce qui a Ã©tÃ© traitÃ©
3. Tester Ã©tape par Ã©tape au lieu du script global
4. VÃ©rifier les permissions des fichiers/dossiers

---

## âœ… Checklist de Mise Ã  Jour

Avant de mettre en production :

- [ ] DonnÃ©es CSV prÃ©sentes dans `Documents/`
- [ ] Environnement Python activÃ©
- [ ] DÃ©pendances installÃ©es (`pip install -r requirements.txt`)
- [ ] Scripts exÃ©cutÃ©s avec succÃ¨s
- [ ] Base de donnÃ©es reconstruite
- [ ] API testÃ©e et fonctionnelle
- [ ] Frontend connectÃ© Ã  l'API

---

**DerniÃ¨re mise Ã  jour :** Novembre 2024
**Version :** 1.0
