#!/usr/bin/env python3
"""
Syst√®me RAG (Retrieval-Augmented Generation) pour BurkinaHeritage
==================================================================

Architecture 100% Open Source :
- Embeddings : ChromaDB DefaultEmbeddingFunction (all-MiniLM-L6-v2)
- Base vectorielle : ChromaDB (stockage local persistant)
- LLM : Hugging Face Inference API (Mistral-7B-Instruct) + Fallback local

Ce module impl√©mente le pipeline RAG complet :
1. Indexation des documents culturels burkinab√®
2. Recherche par similarit√© vectorielle
3. G√©n√©ration de r√©ponses contextualis√©es

Auteur : BurkinaHeritage Team
Date : Novembre 2025
Licence : Open Source
"""

import json
import os
from pathlib import Path
from typing import List, Dict
import chromadb
from chromadb.utils import embedding_functions
import requests

# Charger les variables d'environnement depuis .env
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# Importer Google Generative AI avec la nouvelle syntaxe
try:
    from google import genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è  google-genai non install√©. Installation: pip install google-genai")


class BurkinaHeritageRAGSimple:
    """
    Syst√®me RAG simplifi√© pour r√©pondre aux questions sur la culture burkinab√®.
    
    Ce syst√®me permet de :
    - Charger un corpus de documents culturels
    - Indexer les documents dans une base vectorielle
    - Rechercher les documents pertinents par similarit√© s√©mantique
    - G√©n√©rer des r√©ponses √† partir du contexte r√©cup√©r√©
    
    Attributes:
        corpus (List[Dict]): Liste des documents charg√©s
        collection: Collection ChromaDB pour la recherche vectorielle
        hf_token (str): Token Hugging Face (optionnel)
        hf_api_url (str): URL de l'API Hugging Face
    """
    
    def __init__(
        self,
        corpus_path: str = "data/corpus.json",
        chroma_dir: str = "data/chroma_db"
    ):
        """
        Initialise le syst√®me RAG.
        
        Args:
            corpus_path (str): Chemin vers le fichier JSON contenant le corpus
            chroma_dir (str): R√©pertoire de stockage de ChromaDB
            
        Raises:
            FileNotFoundError: Si le fichier corpus n'existe pas
            json.JSONDecodeError: Si le corpus n'est pas un JSON valide
        """
        self.corpus_path = Path(corpus_path)
        self.chroma_dir = Path(chroma_dir)
        self.chroma_dir.mkdir(parents=True, exist_ok=True)
        
        print("üöÄ Initialisation du syst√®me RAG BurkinaHeritage...")
        
        # Charger le corpus avec limite pour √©conomiser la m√©moire
        print(f"üìö Chargement du corpus: {self.corpus_path}")
        with open(self.corpus_path, 'r', encoding='utf-8') as f:
            full_corpus = json.load(f)
        
        # OPTIMISATION ULTRA pour Render Free (512MB RAM): Limiter √† 100 documents
        max_docs = 100
        if len(full_corpus) > max_docs:
            print(f"‚ö†Ô∏è  Limitation √† {max_docs} documents (au lieu de {len(full_corpus)}) pour optimisation m√©moire Render Free")
            self.corpus = full_corpus[:max_docs]
        else:
            self.corpus = full_corpus
        
        print(f"‚úÖ {len(self.corpus)} documents charg√©s")
        
        # Initialiser ChromaDB avec embeddings RAPIDES
        # OPTIMISATION: Utiliser DefaultEmbeddingFunction (mod√®le pr√©-install√© avec ChromaDB)
        print("üóÑÔ∏è  Initialisation de ChromaDB...")
        
        # Configuration all√©g√©e pour environnements √† faible m√©moire
        import chromadb.config
        settings = chromadb.config.Settings(
            anonymized_telemetry=False,
            allow_reset=True,
            is_persistent=True
        )
        
        self.chroma_client = chromadb.PersistentClient(
            path=str(self.chroma_dir),
            settings=settings
        )
        
        # OPTIMISATION: DefaultEmbeddingFunction = ONNX pr√©-install√© (pas de t√©l√©chargement)
        self.embedding_function = embedding_functions.DefaultEmbeddingFunction()
        
        # Configuration des LLMs (ordre de priorit√©)
        # 1. Gemini API (Google)
        self.gemini_api_key = os.getenv("GEMINI_API_KEY", "")
        if self.gemini_api_key and GEMINI_AVAILABLE:
            # Configurer avec la cl√© API
            os.environ["GEMINI_API_KEY"] = self.gemini_api_key
            self.gemini_client = genai.Client(api_key=self.gemini_api_key)
            print("‚úÖ Gemini API configur√©e")
        else:
            self.gemini_client = None
            if not self.gemini_api_key:
                print("‚ö†Ô∏è  GEMINI_API_KEY non d√©finie (variable d'environnement)")
        
        # 2. Hugging Face (fallback)
        self.hf_token = os.getenv("HUGGINGFACE_TOKEN", "")
        self.hf_api_url = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2"
        
        # Setup collection
        self._setup_collection()
        
        print("‚úÖ Syst√®me RAG initialis√©!\n")
    
    def _setup_collection(self):
        """
        Configure la collection ChromaDB pour stocker les embeddings.
        
        Tente de charger une collection existante, sinon en cr√©e une nouvelle
        et indexe tous les documents du corpus.
        
        Note:
            La collection est nomm√©e "burkina_culture" et stock√©e de mani√®re persistante
        """
        collection_name = "burkina_culture"
        
        try:
            # Essayer de charger la collection existante
            self.collection = self.chroma_client.get_collection(
                name=collection_name,
                embedding_function=self.embedding_function
            )
            print(f"üìÇ Collection charg√©e: {self.collection.count()} documents")
            
        except Exception:
            # Cr√©er une nouvelle collection
            print(f"üÜï Cr√©ation de la collection...")
            self.collection = self.chroma_client.create_collection(
                name=collection_name,
                embedding_function=self.embedding_function,
                metadata={"description": "Culture du Burkina Faso"}
            )
            self._index_documents()
    
    def _index_documents(self):
        """
        Indexe tous les documents du corpus dans ChromaDB.
        
        ULTRA-OPTIMIS√â pour environnements √† tr√®s faible m√©moire (512 MB).
        Les documents sont trait√©s par micro-batches de 20 au lieu de 50.
        
        Process:
            1. Diviser le corpus en micro-batches de 20
            2. Pour chaque batch : extraire texte, m√©tadonn√©es et IDs
            3. Ajouter √† la collection ChromaDB
            4. Lib√©rer agressivement la m√©moire entre chaque batch
            5. Afficher la progression
        """
        print("üîÑ Indexation des documents...")
        
        # OPTIMISATION ULTRA pour Render Free: R√©duire la taille des batches √† 10
        batch_size = 10  # R√©duit de 20 √† 10 pour Render Free (512MB RAM)
        
        for i in range(0, len(self.corpus), batch_size):
            batch = self.corpus[i:i + batch_size]
            
            # Pr√©parer les donn√©es
            documents = [doc["content"] for doc in batch]
            metadatas = [
                {
                    "title": doc["title"],
                    "source": doc["source"],
                    "category": doc["category"]
                }
                for doc in batch
            ]
            ids = [f"doc_{doc['id']}" for doc in batch]
            
            # Ajouter √† ChromaDB
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            # OPTIMISATION MAXIMALE: Lib√©rer agressivement la m√©moire
            del documents, metadatas, ids, batch
            import gc
            gc.collect()
            
            # Afficher progression r√©duite (tous les 100 docs)
            if (i + batch_size) % 100 == 0 or (i + batch_size) >= len(self.corpus):
                print(f"  ‚úì {min(i + batch_size, len(self.corpus))}/{len(self.corpus)} index√©s")
        
        print("‚úÖ Indexation termin√©e!")
    
    def search_documents(self, query: str, n_results: int = 3) -> List[Dict]:
        """
        Recherche les documents les plus pertinents par similarit√© vectorielle.
        
        OPTIMIS√â: n_results=3 au lieu de 5 par d√©faut pour r√©duire la charge m√©moire
        
        Utilise ChromaDB pour trouver les documents dont le contenu est s√©mantiquement
        proche de la requ√™te. Filtre intelligemment par cat√©gorie selon les mots-cl√©s.
        Tente une expansion d'acronymes si la recherche initiale √©choue.
        
        Args:
            query (str): Question ou requ√™te de l'utilisateur
            n_results (int): Nombre de documents √† retourner (d√©faut: 3, r√©duit de 5)
            
        Returns:
            List[Dict]: Liste des documents pertinents avec leur contenu et m√©tadonn√©es
            
        Example:
            >>> rag.search_documents("Qu'est-ce que le balafon ?", n_results=3)
            [{"content": "...", "metadata": {"title": "...", "source": "..."}}]
        """
        # D√©tection intelligente de cat√©gorie
        query_lower = query.lower()
        
        # Mots-cl√©s culturels (on privil√©gie la cat√©gorie culture)
        cultural_keywords = [
            "griot", "balafon", "djemb√©", "kora", "musique", "danse", "tradition",
            "masque", "fespaco", "siao", "artisan", "tissage", "poterie", "bronze",
            "c√©r√©monie", "rite", "anc√™tre", "chef", "roi", "royaume", "ethnie",
            "mossi", "peul", "bobo", "lobi", "gourounsi", "touareg"
        ]
        
        # Mots-cl√©s architecturaux
        architectural_keywords = [
            "grenier", "case", "maison", "habitat", "construction", "architecture",
            "mosqu√©e", "b√¢timent", "√©difice", "banco", "terre", "paille"
        ]
        
        # D√©terminer les cat√©gories √† privil√©gier
        prefer_culture = any(kw in query_lower for kw in cultural_keywords)
        prefer_architecture = any(kw in query_lower for kw in architectural_keywords)
        
        # R√©cup√©rer plus de r√©sultats pour filtrer ensuite
        n_fetch = n_results * 3 if (prefer_culture or prefer_architecture) else n_results
        
        results = self.collection.query(
            query_texts=[query],
            n_results=min(n_fetch, 15)  # Maximum 15 pour performance
        )
        
        documents = []
        for i in range(len(results['documents'][0])):
            doc = {
                "content": results['documents'][0][i],
                "metadata": results['metadatas'][0][i]
            }
            
            # Filtrage intelligent par cat√©gorie
            category = doc['metadata'].get('category', '')
            
            # Si on cherche de la culture, on exclut l'architecture
            if prefer_culture and not prefer_architecture:
                if category == 'architecture':
                    continue
            
            documents.append(doc)
            
            # Arr√™ter quand on a assez de r√©sultats
            if len(documents) >= n_results:
                break
        
        return documents[:n_results]
    
    def generate_answer_hf(self, question: str, context_docs: List[Dict], conversation_history: List[Dict] = None) -> str:
        """
        G√©n√®re une r√©ponse intelligente avec Gemini.
        
        Strat√©gie hybride intelligente :
        - Si contexte trouv√© : Gemini reformule + peut compl√©ter/corriger
        - Si pas de contexte : Gemini r√©pond avec ses propres connaissances
        
        Args:
            question (str): Question de l'utilisateur
            context_docs (List[Dict]): Documents de contexte (peut √™tre vide)
            conversation_history (List[Dict]): Historique de la conversation
            
        Returns:
            str: R√©ponse g√©n√©r√©e par Gemini
        """
        if conversation_history is None:
            conversation_history = []
            
        # V√©rifier si on a du contexte pertinent
        has_context = len(context_docs) > 0
        
        # Construire l'historique pour le prompt (derniers 6 messages max)
        history_text = ""
        if conversation_history and len(conversation_history) > 1:  # Au moins 2 messages (user + assistant)
            recent_history = conversation_history[-7:-1]  # Exclure le dernier (question actuelle)
            if recent_history:
                history_lines = []
                for msg in recent_history:
                    role = "Utilisateur" if msg.get("role") == "user" else "Assistant"
                    content = msg.get("content", "")[:150]  # Limiter la longueur
                    history_lines.append(f"{role}: {content}")
                history_text = "\n".join(history_lines)
        
        if has_context:
            # Construire le contexte depuis la BD
            context = "\n\n".join([
                f"Document {i+1}:\n{doc['content'][:500]}"
                for i, doc in enumerate(context_docs[:3])
            ])
            
            # PROMPT HYBRIDE : Reformuler + Compl√©ter/Corriger AVEC HISTORIQUE
            if history_text:
                prompt = f"""Tu es un assistant expert sur le Burkina Faso (culture, histoire, traditions).

HISTORIQUE DE LA CONVERSATION :
{history_text}

CONTEXTE TROUV√â DANS MA BASE DE DONN√âES :
{context}

QUESTION DE L'UTILISATEUR : {question}

TA MISSION :
1. TIENS COMPTE de l'historique de conversation ci-dessus pour comprendre le contexte
2. Utilise les informations du contexte de ma base de donn√©es comme BASE
3. Reformule de mani√®re claire et fluide (pas de copier-coller)
4. Tu peux COMPL√âTER avec tes propres connaissances si n√©cessaire
5. Si la question fait r√©f√©rence √† quelque chose dans l'historique (comme "elle", "il", "le SIAO", etc.), utilise cet historique
6. R√©ponds de mani√®re naturelle et informative (2-4 phrases)

IMPORTANT : R√©ponds de fa√ßon coh√©rente avec la conversation pr√©c√©dente.

R√âPONSE (en fran√ßais, naturelle et compl√®te) :"""
            else:
                prompt = f"""Tu es un assistant expert sur le Burkina Faso (culture, histoire, traditions).

CONTEXTE TROUV√â DANS MA BASE DE DONN√âES :
{context}

QUESTION DE L'UTILISATEUR : {question}

TA MISSION :
1. Utilise les informations du contexte ci-dessus comme BASE
2. Reformule de mani√®re claire et fluide (pas de copier-coller)
3. Tu peux COMPL√âTER avec tes propres connaissances si n√©cessaire
4. Tu peux CORRIGER si une information semble incorrecte
5. R√©ponds de mani√®re naturelle et informative (2-4 phrases)

IMPORTANT : M√™me si le contexte ne r√©pond pas parfaitement, utilise tes connaissances du Burkina Faso pour donner une r√©ponse compl√®te et utile.

R√âPONSE (en fran√ßais, naturelle et compl√®te) :"""
        else:
            # PAS DE CONTEXTE : Gemini r√©pond en mode conversationnel AVEC HISTORIQUE
            if history_text:
                prompt = f"""Tu es BurkinaHeritage, un assistant sympathique et expert sur le Burkina Faso.

HISTORIQUE DE LA CONVERSATION :
{history_text}

QUESTION : {question}

TA MISSION :
- TIENS COMPTE de l'historique pour comprendre le contexte
- Si la question fait r√©f√©rence √† la conversation pr√©c√©dente, utilise cet historique
- Si c'est une salutation ‚Üí r√©ponds chaleureusement
- Si c'est une question sur le Burkina Faso ‚Üí r√©ponds avec tes connaissances
- Reste naturel, sympathique et coh√©rent avec la conversation
- R√©ponds en fran√ßais (1-3 phrases)

R√âPONSE (naturelle, sympathique et coh√©rente) :"""
            else:
                prompt = f"""Tu es BurkinaHeritage, un assistant sympathique et expert sur le Burkina Faso.

QUESTION : {question}

CONTEXTE : C'est une question conversationnelle ou aucune donn√©e sp√©cifique n'est n√©cessaire.

TA MISSION :
- Si c'est une salutation (bonjour, salut, etc.) ‚Üí r√©ponds chaleureusement et bri√®vement
- Si c'est une question sur toi ‚Üí explique que tu es un assistant sur le Burkina Faso
- Si c'est une question sur le Burkina Faso ‚Üí r√©ponds avec tes connaissances
- Reste naturel, sympathique et concis (1-3 phrases)
- R√©ponds en fran√ßais

R√âPONSE (naturelle et sympathique) :"""
        
        # M√©thode 1: GEMINI API (PRIORIT√â)
        gemini_error_message = None
        if self.gemini_client:
            try:
                response = self.gemini_client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt
                )
                answer = response.text.strip()
                if answer and len(answer) > 30:
                    context_status = "avec contexte BD" if has_context else "sans contexte (Gemini pur)"
                    print(f"‚úÖ R√©ponse g√©n√©r√©e par Gemini ({context_status})")
                    return answer
            except Exception as e:
                error_str = str(e)
                print(f"‚ö†Ô∏è  Erreur Gemini API: {e}")
                
                # D√©terminer le type d'erreur et cr√©er un message appropri√©
                if "503" in error_str or "overloaded" in error_str.lower():
                    gemini_error_message = "‚ö†Ô∏è Le service d'IA est temporairement surcharg√©. Veuillez r√©essayer dans quelques instants."
                elif "429" in error_str or "quota" in error_str.lower() or "rate" in error_str.lower():
                    gemini_error_message = "‚ö†Ô∏è Limite d'utilisation atteinte. Veuillez r√©essayer dans quelques minutes."
                elif "401" in error_str or "unauthorized" in error_str.lower() or "api key" in error_str.lower():
                    gemini_error_message = "‚ö†Ô∏è Probl√®me de configuration de l'API. Veuillez contacter l'administrateur."
                elif "network" in error_str.lower() or "connection" in error_str.lower():
                    gemini_error_message = "‚ö†Ô∏è Probl√®me de connexion r√©seau. Veuillez v√©rifier votre connexion internet et r√©essayer."
                else:
                    gemini_error_message = "‚ö†Ô∏è Le service d'IA est temporairement indisponible. Veuillez r√©essayer ult√©rieurement."
        
        # Fallback si Gemini √©choue
        if has_context:
            # Si on a du contexte mais Gemini a √©chou√©, utiliser le fallback intelligent
            print("‚ö†Ô∏è  Utilisation du fallback avec contexte")
            fallback_answer = self._fallback_answer(context_docs, question)
            # Ajouter le message d'erreur au d√©but si Gemini a √©chou√©
            if gemini_error_message:
                return f"{gemini_error_message}\n\nVoici les informations que j'ai trouv√©es dans ma base de donn√©es :\n\n{fallback_answer}"
            return fallback_answer
        else:
            # NOUVEAU : Essayer de chercher avec des termes plus g√©n√©raux
            print("‚ö†Ô∏è  Pas de contexte trouv√©, recherche √©largie...")
            
            # Termes g√©n√©raux sur le Burkina Faso
            general_terms = [
                "Burkina Faso culture traditions",
                "histoire Burkina Faso",
                "patrimoine burkinab√®"
            ]
            
            expanded_docs = []
            for term in general_terms:
                try:
                    results = self.collection.query(
                        query_texts=[term],
                        n_results=3
                    )
                    if results and results['documents'] and results['documents'][0]:
                        for i in range(len(results['documents'][0])):
                            expanded_docs.append({
                                "content": results['documents'][0][i],
                                "metadata": results['metadatas'][0][i]
                            })
                except Exception as e:
                    continue
            
            if expanded_docs:
                print(f"‚úÖ Trouv√© {len(expanded_docs)} documents g√©n√©raux")
                fallback_answer = self._fallback_answer(expanded_docs[:5], question)
                # Ajouter le message d'erreur au d√©but si Gemini a √©chou√©
                if gemini_error_message:
                    return f"{gemini_error_message}\n\nVoici des informations g√©n√©rales sur le Burkina Faso :\n\n{fallback_answer}"
                return fallback_answer
            else:
                print("‚ö†Ô∏è  Aucun document trouv√© m√™me avec recherche √©largie")
                error_prefix = f"{gemini_error_message}\n\n" if gemini_error_message else ""
                return f"{error_prefix}D√©sol√©, je n'ai pas d'information sur ce sujet dans ma base de donn√©es. Posez-moi des questions sur la culture, l'histoire, les traditions, l'artisanat ou l'architecture du Burkina Faso. Par exemple : 'Qu'est-ce que le SIAO ?', 'Parle-moi du FESPACO', 'Qui est Thomas Sankara ?'"
    
    def _fallback_answer(self, context_docs: List[Dict], question: str = "") -> str:
        """
        G√©n√®re une r√©ponse reformul√©e bas√©e sur le contexte (sans LLM externe).
        
        Cette fonction cr√©e une synth√®se intelligente en:
        1. Extrayant les passages les plus pertinents
        2. Les combinant de mani√®re fluide
        3. Structurant la r√©ponse de mani√®re compr√©hensible
        
        Args:
            context_docs (List[Dict]): Documents de contexte
            question (str): Question pos√©e par l'utilisateur
            
        Returns:
            str: R√©ponse reformul√©e et structur√©e
        """
        if not context_docs:
            return "D√©sol√©, je n'ai pas trouv√© d'information sur ce sujet dans ma base de donn√©es. Posez-moi des questions sur la culture, l'histoire, les traditions ou l'architecture du Burkina Faso."

        # Extraire les contenus des meilleurs documents
        best_docs = context_docs[:3]
        
        # D√©tecter le type de question
        question_lower = question.lower()
        is_what_question = any(word in question_lower for word in ["qu'est-ce", "c'est quoi", "what is", "d√©finition"])
        is_general_culture = any(word in question_lower for word in ["culture", "traditions", "patrimoine", "burkinab"])
        
        # Construire une introduction contextuelle
        intro = ""
        if is_what_question:
            intro = "Voici ce que je peux vous dire : "
        elif is_general_culture:
            intro = "Concernant la culture burkinab√® : "
        
        # Combiner les contenus de mani√®re intelligente
        combined_content = []
        total_words = 0
        max_words = 250  # Limite pour √©viter les r√©ponses trop longues
        
        for doc in best_docs:
            content = doc.get('content', '').strip()
            if not content:
                continue
            
            # Diviser en phrases (g√©rer plusieurs d√©limiteurs)
            import re
            sentences = re.split(r'[.!?]\s+', content)
            
            # Ajouter les phrases les plus pertinentes
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 25:  # Ignorer les phrases trop courtes
                    continue
                
                # √âviter les r√©p√©titions
                if sentence in combined_content:
                    continue
                
                words = sentence.split()
                if total_words + len(words) > max_words:
                    break
                
                combined_content.append(sentence)
                total_words += len(words)
                
                if len(combined_content) >= 4:  # Maximum 4 phrases
                    break
            
            if total_words >= max_words or len(combined_content) >= 4:
                break
        
        if not combined_content:
            # Fallback ultime - prendre le premier document
            first_content = best_docs[0].get('content', '')
            if len(first_content) > 400:
                return intro + first_content[:400].strip() + "..."
            return intro + first_content
        
        # Assembler la r√©ponse de mani√®re fluide
        answer = intro + ' '.join(combined_content)
        
        # Nettoyer et formater
        answer = answer.strip()
        
        # S'assurer que la r√©ponse se termine bien
        if not answer.endswith(('.', '!', '?')):
            answer += '.'
        
        # Limiter la longueur totale
        if len(answer) > 600:
            answer = answer[:597] + "..."
        
        return answer
    
    def _needs_database_search(self, question: str) -> bool:
        """
        D√©termine si la question n√©cessite une recherche dans la base de donn√©es.
        
        Args:
            question (str): Question de l'utilisateur
            
        Returns:
            bool: True si recherche BD n√©cessaire, False sinon
        """
        question_lower = question.lower().strip()
        
        # Mots-cl√©s qui indiquent un besoin de recherche documentaire
        keywords_requiring_search = [
            # Culture
            "griot", "balafon", "djemb√©", "kora", "musique", "danse", "tradition",
            "masque", "fespaco", "siao", "artisan", "tissage", "poterie", "bronze",
            "c√©r√©monie", "rite", "anc√™tre", "chef", "roi", "royaume", "ethnie",
            "mossi", "peul", "bobo", "lobi", "gourounsi", "touareg",
            # Architecture
            "grenier", "case", "maison", "habitat", "construction", "architecture",
            "mosqu√©e", "b√¢timent", "√©difice", "banco", "terre", "paille",
            # Histoire
            "histoire", "ind√©pendance", "thomas sankara", "sankara", "mogho naba",
            "empire", "colonial", "fran√ßais", "guerre",
            # G√©ographie/Lieux
            "ouagadougou", "bobo-dioulasso", "banfora", "ville", "r√©gion",
            # Questions explicites
            "qui est", "qu'est-ce que", "c'est quoi", "parle-moi de",
            "explique", "raconte", "d√©finition", "signification"
        ]
        
        # Si un mot-cl√© est d√©tect√© ‚Üí recherche BD
        return any(keyword in question_lower for keyword in keywords_requiring_search)
    
    def _simple_chat_response(self, question: str) -> str:
        """
        R√©ponse simple sans recherche BD pour conversation g√©n√©rale.
        
        Args:
            question (str): Question de l'utilisateur
            
        Returns:
            str: R√©ponse conversationnelle
        """
        question_lower = question.lower().strip()
        
        # Salutations
        greetings = ["bonjour", "bonsoir", "salut", "hello", "yo", "hey", "coucou"]
        if any(greeting in question_lower for greeting in greetings):
            return """Bonjour ! üëã

Je suis BurkinaHeritage, votre assistant culturel sur le Burkina Faso.

Posez-moi des questions sur la culture, l'histoire, les traditions, l'architecture... Je suis l√† pour vous aider ! üòä"""
        
        # Questions sur l'√©tat
        if any(q in question_lower for q in ["comment tu vas", "√ßa va", "comment allez-vous"]):
            return "Je vais tr√®s bien, merci ! üòä Pr√™t √† r√©pondre √† vos questions sur le Burkina Faso !"
        
        # Questions sur l'identit√©
        if any(q in question_lower for q in ["qui es-tu", "qui √™tes-vous", "ton nom", "tu es qui"]):
            return """Je suis **BurkinaHeritage**, un assistant culturel sp√©cialis√© dans le patrimoine du Burkina Faso. üáßüá´

Je dispose de 370 documents sur la culture, l'architecture, l'histoire et bien plus encore.

Posez-moi des questions pr√©cises pour en savoir plus ! üìö"""
        
        # Questions sur les capacit√©s
        if any(q in question_lower for q in ["que sais-tu", "que connais-tu", "que peux-tu", "tu connais quoi"]):
            return """Je connais beaucoup de choses sur le **Burkina Faso** ! üáßüá´

Mes domaines d'expertise :
‚Ä¢ üé≠ Culture (traditions, griots, musique, artisanat)
‚Ä¢ üèõÔ∏è Architecture traditionnelle
‚Ä¢ üìö Histoire et grands personnages
‚Ä¢ üé¨ √âv√©nements culturels (FESPACO, SIAO...)
‚Ä¢ üåç Patrimoine et soci√©t√©

Posez-moi une question sp√©cifique ! üòä"""
        
        # R√©ponse par d√©faut pour conversation g√©n√©rale
        return """Je suis sp√©cialis√© dans le patrimoine du Burkina Faso. 

Pour que je puisse vous aider au mieux, posez-moi une question pr√©cise sur :
‚Ä¢ La culture et les traditions
‚Ä¢ L'histoire du pays
‚Ä¢ L'architecture
‚Ä¢ Les personnalit√©s marquantes
‚Ä¢ Les √©v√©nements culturels

Que voulez-vous savoir ? ü§î"""
    
    def ask(self, question: str, use_llm: bool = True, conversation_history: List[Dict] = None) -> Dict:
        """
        Point d'entr√©e principal : TOUT passe par Gemini.
        
        Pipeline ultra-intelligent :
        1. Recherche dans la BD pour avoir du contexte
        2. Gemini r√©pond TOUJOURS (conversation + reformulation + compl√©tion)
        3. Sources affich√©es seulement si pertinentes
        
        Args:
            question (str): Question de l'utilisateur
            use_llm (bool): Toujours True (Gemini intelligent)
            conversation_history (List[Dict]): Historique de la conversation
            
        Returns:
            Dict: Dictionnaire contenant question, answer, sources
        """
        if conversation_history is None:
            conversation_history = []
            
        print(f"\n‚ùì Question: {question}")
        if conversation_history:
            print(f"üìú Historique: {len(conversation_history)} messages")
        
        # OPTIMISATION: G√©rer les salutations et questions simples AVANT Gemini
        question_lower = question.lower().strip()
        simple_greetings = ["bonjour", "salut", "bonsoir", "coucou", "hey", "hello", "hi"]
        
        if question_lower in simple_greetings or any(g == question_lower for g in simple_greetings):
            print("üëã Salutation d√©tect√©e - R√©ponse directe")
            return {
                "question": question,
                "answer": "Bonjour ! üëã Je suis BurkinaHeritage, votre assistant culturel sur le Burkina Faso. Comment puis-je vous aider aujourd'hui ? üòä",
                "sources": []
            }
        
        # D√©terminer si on recherche dans la BD
        needs_db = self._needs_database_search(question)
        
        if needs_db:
            # Question sp√©cifique ‚Üí rechercher dans la BD
            print("üîç Recherche dans la base de donn√©es...")
            docs = self.search_documents(question, n_results=5)
            
            if docs:
                print(f"‚úÖ {len(docs)} documents trouv√©s")
            else:
                print("‚ö†Ô∏è  Aucun document trouv√©")
        else:
            # Question conversationnelle ‚Üí pas de recherche BD
            print("üí¨ Question conversationnelle")
            docs = []
        
        # GEMINI R√âPOND TOUJOURS (conversation + reformulation + compl√©tion)
        print("ü§ñ Gemini g√©n√®re la r√©ponse...")
        answer = self.generate_answer_hf(question, docs, conversation_history)
        
        # Sources (seulement si on a cherch√© dans la BD ET trouv√© des docs)
        sources = []
        if needs_db and docs:
            sources = [
                {
                    "title": doc['metadata']['title'],
                    "source": doc['metadata']['source'],
                    "category": doc['metadata']['category']
                }
                for doc in docs[:3]
            ]
        
        # Ajouter les sources APR√àS la r√©ponse (seulement si pertinentes)
        answer_with_sources = answer.strip()
        if sources:
            source_lines = "\n".join([f"- {s['source']}" for s in sources])
            answer_with_sources = f"{answer_with_sources}\n\n\nüìö Sources :\n\n{source_lines}"

        return {
            "question": question,
            "answer": answer_with_sources,
            "sources": sources
        }


def main():
    """Test"""
    print("\n" + "=" * 70)
    print("üáßüá´ BurkinaHeritage RAG - Test")
    print("=" * 70 + "\n")
    
    rag = BurkinaHeritageRAGSimple()
    
    questions = [
        "Qu'est-ce que le balafon ?",
        "Parle-moi de l'architecture au Burkina Faso",
        "Qui sont les griots ?"
    ]
    
    for i, q in enumerate(questions, 1):
        print(f"\n{'='*70}")
        print(f"TEST {i}/{len(questions)}")
        print("="*70)
        
        result = rag.ask(q, use_llm=False)  # Sans LLM pour le test
        
        print(f"\nüìù R√âPONSE:")
        print("-" * 70)
        print(result['answer'][:500])
        
        print(f"\nüìö SOURCES:")
        for j, s in enumerate(result['sources'], 1):
            print(f"  {j}. {s['source']}")
    
    print("\n‚úÖ Tests termin√©s!\n")


if __name__ == "__main__":
    main()
